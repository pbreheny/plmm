---
title: "Exploring different modeling options"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(penalizedLMM)
library(data.table)
library(ggplot2)
```

The `plmm()` function has many options, so this article aims to provide an overview (and comparison) of several of these model fitting options. This is a more detailed presentation than what is provided in the "Getting started" vignette.

# Options for K matrix

One of the 'coolest' aspects of the `plmm` approach is its ability to incorporate complex relationships into the model-fitting process. In our framework, the relational structure of a data set is represented in the matrix we call $\mathbf{K}$. Imagine this matrix as a quilt with a symmetric, checkered pattern. The more complex the relational structure of the data, the more intricate the pattern of the quilt becomes. The relational structure in a data set could include broad and loosely related groups, (like ancestry groups), or smaller and highly correlated groups (like family units).

## Structure

As the number of observations (e.g., number of participants) increases, the computational time needed to calculate the entire $\mathbf{K}$ increases exponentially. For this reason, `plmm()` has several options for approximating $\mathbf{K}$. First, there are research contexts in which it may be appropriate to assume the data are from unrelated observations. For such cases, `plmm()` offers the argument `diag_K = T`. This will use the simplest possible 'quilt pattern', in which $\mathbf{K} = \mathbf{I}_n$ with $n$ being the number of observations.

We can visualize the impact of two approaches for choosing $\mathbf{K}$: on the left is the default option (as in `relatedness_mat()`), and on the right is the option `diag_K = T`.

```{r diag_K}
# add this plot 
fit_admix1 <- plmm(X = admix$X, y = admix$y)
fit_admix2 <- plmm(X = admix$X, y = admix$y, diag_K = TRUE)
par(mfrow=c(1,2))
plot(fit_admix1);plot(fit_admix2)
```

```{r, warning = FALSE, eval=FALSE, include=FALSE}
# TODO: should I add these plots? 
K_dim <- c(100, 1000, 1500, 2000, 3000, 4000)
seeds <- 1:length(K_dim)
diag_times <- rel_times <- rep(NA, length(K_dim))
pb <- txtProgressBar(min = 1, max = length(K_dim), initial = 1, style = 3)
for (i in 1:length(K_dim)){
  n <- K_dim[i]
  set.seed(seeds[i])
  my_X <- matrix(rnorm(n, 10), n, 10)
  set.seed(seeds[i])
  my_y <- rnorm(n)
  diag_times[i] <- plmm(X = my_X,
                        y = my_y,
                        diag_K = TRUE) |> system.time() 
  
  rel_times[i] <- plmm(X = my_X,
                          y = my_y,
                          K = relatedness_mat(X = my_X)) |>  system.time() 
  setTxtProgressBar(pb, i)
}

K_dat2 <- data.frame(rel_times, diag_times, K_dim)
ggplot(K_dat2,
       aes(x = K_dim)) + 
  geom_line(aes(y = rel_times)) + 
  geom_line(aes(y = diag_times), linetype = 'dashed') + 
  labs(x = "nrow(X): the dimension of the K matrix",
       y = "PLMM time in seconds")


```

## SVD implementation

The computation associated with calculating $\mathbf{K}$ can become quite expensive, as the code below illustrates:

```{r svd_example, warning=FALSE}
#TODO: add these plots 
K_dim <- c(100, 1000, 1500, 2000, 3000, 4000)
true_times <- approx_times <- rep(NA, length(K_dim))
pb <- txtProgressBar(min = 1, max = length(K_dim), initial = 1, style = 3)
for (i in 1:length(K_dim)){
  n <- K_dim[i]
  true_times[i] <- svd(relatedness_mat(X = matrix(rnorm(n, 10), n, 10))) |> 
    system.time() 
  approx_times[i] <- RSpectra::svds(relatedness_mat(X = matrix(rnorm(n, 10), n, 10)),
                                    k = floor(0.1*n)) |> system.time() 
  setTxtProgressBar(pb, i)
}

K_dat <- data.frame(true_times, approx_times, K_dim)
ggplot(K_dat,
       aes(x = K_dim)) + 
  geom_line(aes(y = true_times)) + 
  geom_line(aes(y = approx_times), linetype = 'dashed') + 
  labs(x = "nrow(X): the dimension of the K matrix",
       y = "SVD time in seconds")
```

You can use `choose_k()` to implement truncated singular value decomposition (SVD). This is what is shown above via `RSpectra::svds()`.

# Options for 'type' in prediction

In PLMMs (as well as other contexts), a goal in model building is to predict outcomes in new data $X_2$ based on data and outcomes $X_1, y_1$. Our package offers two options for prediction, 'lp' and 'blup', which the user may specify via the `type` argument in `cv.plmm()` and related functions. LP is an acronym for "Linear Predictor" and BLUP is an acronym for Best Linear Unbiased Predictor. The linear predictor *(LP)* is $X_2 \hat \beta(\lambda)$, whereas the Best Linear Unbiased Predictor (*BLUP*) is $X_2 \hat \beta(\lambda) + V_{21} V_{11}^{\frac{-1}{2}}(\tilde y_1 - \tilde X_1 \hat \beta(\lambda))$, where:

-   $V_{n \times n} \equiv \eta \mathbf{K} + (1 - \eta)\mathbf{I}_n$ is the matrix describing the correlation among the $n$ observations of $X$. This matrix could be re-expressed as $V_{n \times n} \equiv \eta*(U \text{diag}(S)U^\top) + (1 - \eta)I_n$ via singular value decomposition.

-   $\tilde X_1$ and $\tilde y_1$ are the standardized and rotated design matrix and outcomes.

-   (*This bullet is a work in progress... need to think about this some more.*) The symbol $\eta$ represents the percentage of $\text{Var}(y)$ attributable to additive effects of the features (in the context of genetics, this is narrow-sense heritability). $\eta$ also captures the signal-to-noise ratio (SNR) of the data-generating mechanism (model). We estimate $\eta$ as $\hat \eta = \text{argmin} \ell (U, S, y)$, the value that minimizes the log-likelihood function (see `estimate_eta()` documentation and details).

Note that the BLUP includes the LP as its first term, and adds a second term representing the predicted random effect. This means that the BLUP accounts for the covariance structure in the data, whereas the LP does not incorporate that structure.

We can compare results from these options here:

```{r blup_example}
set.seed(615)
cv_admix1 <- cv.plmm(X = admix$X, y = admix$y)
set.seed(615)
cv_admix1_blup <- cv.plmm(X = admix$X, y = admix$y, type = "blup")

par(mfrow=c(1,2)); plot(cv_admix1); plot(cv_admix1_blup)
summary(cv_admix1);summary(cv_admix1_blup)
```

# Options for looking at bias and variance

It may be of interest to examine the bias and variance that result from cross-validated model fitting. `cv.plmm()` has the option `returnBiasDetails`; if set to `TRUE`, this will make the returned object include bias (a vector) and loss (a matrix). Moreover, for a `plmm` model fit, the function `v_hat` will construct the approximated matrix $\mathbf{\hat V} \equiv \eta \mathbf{K} + (1 - \eta)\mathbf{I}_n$.
