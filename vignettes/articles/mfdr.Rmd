---
title: "Inference with marginal false discovery rates"
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(penalizedLMM)
library(ggplot2)
library(dplyr)
library(tibble)
```

## Background and motivation

While cross validation (as in `cv.plmm()`) serves as a method for assessing a model's ability to predict outcomes in new data, cross validation is not geared towards making *inferences* about the $\hat \beta$ values (coefficients) estimated by a model. When we use a PLMM, we often want to answer questions like:

-   "How reliable are the selections made by the model I chose?"

-   "How accurate are the estimated $\hat \beta$ values?"

-   "Can we assess which of the features (e.g., which SNPs, or which predictors) are most likely to be false discoveries, and which features are most likely to be truly connected to the outcome I care about?"

One way to address these important questions is through **marginal false discovery rates** (mFdr). In the mFdr framework, I can estimate an mFdr value for each feature in a penalized regression model. Using these mFdr values, I can then rank the selected features on a scale from 0 to 1. A feature with an mFdr value closer to zero has a high probability of being associated with the outcome of interest, whereas a feature with an mFdr value closer to 1 is probably a false discovery. In addition to ranking features, using mFdr also gives me another way to compare different models. Instead of just comparing the cross-validation error, I can also compare the features chosen and the number of probable false discoveries in each model.

For those interested in the statistical theory behind this method, I recommend my advisor's [free online lecture notes](https://myweb.uiowa.edu/pbreheny/7240/s23/notes/3-30.pdf). In this article, I will use the `mfdr()` function to illustrate how marginal false discovery rates can be used to compare models fit with `plmm()` and `cv.plmm()`.

## Examples

### Admix data

Let's begin with a simple example using our semi-simulated [admix data](man/admix.Rd). I will compare 3 models: 

  * fit 1: a model with all singular values and no fixed effects. 
  * fit 2: a model with only $k$ singular values and no fixed effects. 
  * fit 3: a model with only $k$ singular values and top 4 principal components as fixed effects. 
  
  
For each fit, I will use cross validation to select the value of $\lambda$ that is best for prediction. 

```{r fit1}
# construct a cross-validated lasso model 
cv_fit1 <- cv.plmm(X = admix$X,
              y = admix$y,
              penalty = "lasso",
              # remember to set a seed for reproducibility 
              seed = 26)

# look at results 
fit1 <- cv_fit1$fit
summary(cv_fit1)
# Calculate mFdr
# head(mfdr(fit1))
mfdr1 <- mfdr(fit1)[cv_fit1$min,]

```
  

```{r fit2}
# choose k 
admix$k <- choose_k(X = admix$X, returnKapprox = T)

# construct the model 
cv_fit2 <- cv.plmm(X = admix$X,
                   y = admix$y,
                   K = admix$k$K_svd,
                   penalty = "lasso",
                   seed = 26)

# look at results 
fit2 <- cv_fit2$fit
summary(cv_fit2)
# Calculate mFdr
# head(mfdr(fit2))
mfdr2 <- mfdr(fit2)[cv_fit2$min,]
```

```{r pca}
# standardize design matrix and remove constant features 
std_X <- ncvreg::std(admix$X)
# calculate PCs from *standardized* data
pca <- prcomp(std_X, center = TRUE, scale = TRUE)
# plot the top 10 PCs in a scree plot 
plot(x = 1:10,
     y = 100 * proportions(pca$sdev[1:10]^2),
     type = 'b',
     ylab = 'Proportion of variance explained',
     xlab = 'PC',
     main = 'Scree Plot')
# first 4 PCs explain most of the variance, so we will use these as fixed effects; this makes sense given the ancestry of the participants 
pca_dat <- data.frame(race = admix$race, PC1 = pca$x[,1], PC2 = pca$x[, 2])
pca_plot <- ggplot(pca_dat, aes(x = PC1, y = PC2, col = race)) +
  geom_point() +
  coord_fixed()
plot(pca_plot)

PCs <- pca$x
# look at the PCs 
# pca$x[1:5, 1:4]
# incorporate 4 PCs as fixed effects 
admix$X_plus_PCs <- cbind(PCs[,1:4], admix$X)

```

With the principal components calculated, we can now fit our third model: 

```{r fit3}
# choose k using new design matrix
admix$k_plus_PCs <- choose_k(X = admix$X_plus_PCs, returnKapprox = T)

# construct the model 
cv_fit3 <- cv.plmm(X = admix$X_plus_PCs,
                   y = admix$y,
                   K = admix$k_plus_PCs$K_svd,
                   penalty = "lasso",
                   # make sure not to penalize fixed effects - we want to keep 
                   #  these in the model!
                   penalty.factor = c(rep(0, 4), rep(1, ncol(admix$X))),
                   seed = 26)

# look at results 
fit3 <- cv_fit3$fit
summary(cv_fit3)
# Calculate mFdr
# head(mfdr(fit3))
mfdr3 <- mfdr(fit3)[cv_fit3$min,]
```

Let's compare the results from these three models using the admix data: 

```{r}
# columns are ordered by models 1-3
# rows are CV plots & coefficient path plots
par(mfrow = c(2,3))
plot(cv_fit1); plot(cv_fit2); plot(cv_fit3)
plot(fit1); plot(fit2); plot(fit3)

admix_comparison <- dplyr::bind_rows(mfdr1, mfdr2, mfdr3) |> 
  rownames_to_column(var = "lambda") |> 
  mutate(across(.cols = c(EF, mFDR),
                .fns = ~round(.x, digits = 3)))
kable(admix_comparison)


```


Marginal false discovery rates also give us another way to select $\lambda$: instead of choosing the $\lambda$ value that minimizes cross validation error, I could choose the value that limits the mFdr at a certain level. We can examine this approach for the models we fit above: 
```{r}
# let's look at the smallest lambda for which mFdr is < 10% in each fit 

# fit 1 
bestlam1 <- fit1$lambda[mfdr1$mFDR < 0.1] |> min() |> round(5)
mfdr1_res <- mfdr1[as.character(bestlam1),]

# fit 2 


# fit 3 


```


### Maximal cranial width data (high dimensional)

## References & acknowledgements

The `mfdr()` function I have incorporated here in our `penalizedLMM` package is based on Ryan Miller's joint work with Patrick Breheny in the `ncvreg::mfdr()` package. To read more about their work, see [Miller & Breheny (2023) in Stats in Medicine](https://doi.org/10.1002/sim.9678).
