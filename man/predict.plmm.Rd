% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/predict-plmm.R
\name{predict.plmm}
\alias{predict.plmm}
\title{Predict method for plmm class}
\usage{
\method{predict}{plmm}(
  object,
  newX,
  type = c("response", "individual", "coefficients", "vars", "nvars"),
  lambda,
  which = 1:length(object$lambda),
  XX,
  y,
  U,
  S,
  eta,
  covariance,
  intercept = TRUE,
  ...
)
}
\arguments{
\item{object}{An object of class plmm.}

\item{newX}{Design matrix used for computing predicted values if requested.}

\item{type}{A character argument indicating what should be returned.}

\item{lambda}{A numeric vector of lambda values.}

\item{which}{Vector of lambda indices for which coefficients to return.}

\item{XX}{Optional argument. Original design matrix (not including intercept column) from object. Required if \code{type == 'individual'}.}

\item{y}{Optional argument. Original continuous outcome vector from object. Required if \code{type == 'individual'}.}

\item{U}{Optional argument. Eigenvectors from the similarity matrix from object. Required if \code{type == 'individual'}.}

\item{S}{Optional argument. Eigenvalues from the similarity matrix from object. Required if \code{type == 'individual'}.}

\item{eta}{Optional argument. Estimated $eta$ value from object. Required if \code{type == 'individual'}.}

\item{covariance}{Optional argument. $q times n$ covariance matrix between new and old observations. Required if \code{type == 'individual'}.}

\item{intercept}{optional logical argument}

\item{...}{Additional optional arguments}
}
\description{
Predict method for plmm class
}
\examples{
\dontrun{
# Out of sample predictions are NOT improved by BLUP when X cannot be used to
# estimate V
n <- 210
p <- 1000
p1 <- 50
SNR <- 10

# Generate data (X unrelated to V)
V <- Matrix::bdiag(matrix(0.5, n/3, n/3),
matrix(0.5, n/3, n/3),
matrix(0.5, n/3, n/3))
V <- as.matrix(V)
diag(V) <- 1
set.seed(7)
X1 <- matrix(rnorm(n * p, 0, 1), n, p)
j <- 1:p
s <- rep(1, times = p)
b <- j <= p1
b <- b * s
covX <- stats::var(X1) * (n - 1)/n
beta <- b * sqrt(SNR) / sqrt(drop(crossprod(b, covX) \%*\% b))
Xbeta <- X1 \%*\% beta
e <- MASS::mvrnorm(n=1, rep(0, n), V) # errors are structured
y1 <- Xbeta + e

# fit the model
object <- cv.plmm(X1,
                  y1,
                  V,
                  eta_star = 1,
                  type = 'individual',
                  penalty = "lasso",
                  alpha = 1,
                  standardizeX = TRUE,
                  standardizeRtX = TRUE,
                  rotation = TRUE,
                  returnX = TRUE,
                  nfolds = 5)
# NOTE: in-sample predictions as above in CV are fine, because V supplies the
# known covariance between the outcomes used in different folds.

# generate an X2 that is similar to X1
X2 <- X1 + rnorm(nrow(X1), 0, 0.0001)
covX <- stats::var(X2) * (n - 1)/n
beta <- b * sqrt(SNR) / sqrt(drop(crossprod(b, covX) \%*\% b))
Xbeta <- X2 \%*\% beta
e <- MASS::mvrnorm(n=1, rep(0, n), V)
y2 <- Xbeta + e

# linear predictor works well
linear_predictor <- predict.plmm(object$fit,
newX = X2,
type = 'response',
lambda = object$lambda.min)
drop(crossprod(linear_predictor - y2)/length(y2))

# but the blup doesn't, because `cov(t(X2), t(X1))` doesn't provide
# information about `cov(y2, y1)`
blup <- predict.plmm(object$fit,
newX = X2,
type = 'individual',
lambda = object$lambda.min,
XX = X1,
y = y1,
U = object$fit$U,
S = object$fit$S,
covariance = cov(t(X2), t(X1)),
eta = object$fit$eta)

drop(crossprod(blup - y2)/length(y2))

# in this case, cov(y2, y1) ~= V -this information improves the blup
blup_V <- predict.plmm(object$fit,
newX = X2,
type = 'individual',
lambda = object$lambda.min,
XX = X1,
y = y1,
U = object$fit$U,
S = object$fit$S,
covariance = V,
eta = object$fit$eta)

drop(crossprod(blup_V - y2)/length(y2))

# with the null model...
object$fit$beta <- matrix(0, nrow = nrow(object$fit$beta),
ncol = ncol(object$fit$beta))

# ...the linear predictor is worse...
linear_predictor_null <- predict.plmm(object$fit,
newX = X2,
type = 'response',
lambda = object$lambda.min)

drop(crossprod(linear_predictor_null - y2)/length(y2))

# ...but the blup is even worse, because we are left only with a noisy
# rotation of y as an estimate
blup_null <- predict.plmm(object$fit,
newX = X2,
type = 'individual',
lambda = object$lambda.min,
XX = X1,
y = y1,
U = object$fit$U,
S = object$fit$S,
covariance = cov(t(X2), t(X1)),
eta = object$fit$eta)

drop(crossprod(blup_null - y2)/length(y2))

# again, in this case, cov(y2, y1) ~= V, which notably improves the blup
blup_null_V <- predict.plmm(object$fit,
newX = X2,
type = 'individual',
lambda = object$lambda.min,
XX = X1,
y = y1,
U = object$fit$U,
S = object$fit$S,
covariance = V,
eta = object$fit$eta)

drop(crossprod(blup_null_V - y2)/length(y2))



# Based on this, we expect to see out of sample predictions ARE improved when
# V can be estimated from X
n <- 210
p <- 1000
p1 <- 50
SNR <- 10

# Generate data (X related to V)
V <- Matrix::bdiag(matrix(0.5, n/3, n/3),
matrix(0.5, n/3, n/3),
matrix(0.5, n/3, n/3))
V <- as.matrix(V)
diag(V) <- 1
set.seed(7)
X1 <- t(MASS::mvrnorm(n = p, mu = rep(0, n), Sigma = V))
j <- 1:p
s <- rep(1, times = p)
b <- j <= p1
b <- b * s
covX <- stats::var(X1) * (n - 1)/n
beta <- b * sqrt(SNR) / sqrt(drop(crossprod(b, covX) \%*\% b))
Xbeta <- X1 \%*\% beta
e <- MASS::mvrnorm(n=1, rep(0, n), V)
y1 <- Xbeta + e

# fit the model
object <- cv.plmm(X1,
                  y1,
                  V,
                  eta_star = 1,
                  type = 'individual',
                  penalty = "lasso",
                  alpha = 1,
                  standardizeX = TRUE,
                  standardizeRtX = TRUE,
                  rotation = TRUE,
                  returnX = TRUE,
                  nfolds = 5)

# generate an X2 that is similar to X1
X2 <- X1 + rnorm(nrow(X1), 0, 0.0001)
covX <- stats::var(X2) * (n - 1)/n
beta <- b * sqrt(SNR) / sqrt(drop(crossprod(b, covX) \%*\% b))
Xbeta <- X2 \%*\% beta
e <- MASS::mvrnorm(n=1, rep(0, n), V)
y2 <- Xbeta + e

# linear predictor works well
linear_predictor <- predict.plmm(object$fit,
newX = X2,
type = 'response',
lambda = object$lambda.min)
drop(crossprod(linear_predictor - y2)/length(y2))

# and the blup improves upon the linear predictor because
# `cov(t(X2), t(X1))` ~=`cov(y2, y1)`
blup <- predict.plmm(object$fit,
newX = X2,
type = 'individual',
lambda = object$lambda.min,
XX = X1,
y = y1,
U = object$fit$U,
S = object$fit$S,
covariance = cov(t(X2), t(X1)),
eta = object$fit$eta)

drop(crossprod(blup - y2)/length(y2))

# in this case, knowing the true(ish) cov(y2, y1) is even more beneficial than
# using the version estimated from `cov(t(X2), t(X1))`
blup_V <- predict.plmm(object$fit,
newX = X2,
type = 'individual',
lambda = object$lambda.min,
XX = X1,
y = y1,
U = object$fit$U,
S = object$fit$S,
covariance = V,
eta = object$fit$eta)

drop(crossprod(blup_V - y2)/length(y2))

# with the null model...
object$fit$beta <- matrix(0, nrow = nrow(object$fit$beta),
ncol = ncol(object$fit$beta))

# ...the linear predictor is worse...
linear_predictor_null <- predict.plmm(object$fit,
newX = X2,
type = 'response',
lambda = object$lambda.min)

drop(crossprod(linear_predictor_null - y2)/length(y2))

# ...but the blup is better because we still have information about
# `cov(y2, y1)` ~= `cov(t(X2), t(X1))`
blup_null <- predict.plmm(object$fit,
newX = X2,
type = 'individual',
lambda = object$lambda.min,
XX = X1,
y = y1,
U = object$fit$U,
S = object$fit$S,
covariance = cov(t(X2), t(X1)),
eta = object$fit$eta)

drop(crossprod(blup_null - y2)/length(y2))

# in this case, knowing the true(ish) cov(y2, y1) gives a more drastic
# improvement than using the version estimated from `cov(t(X2), t(X1))`
# because we don't have any information from the linear predictor
blup_null_V <- predict.plmm(object$fit,
newX = X2,
type = 'individual',
lambda = object$lambda.min,
XX = X1,
y = y1,
U = object$fit$U,
S = object$fit$S,
covariance = V,
eta = object$fit$eta)

drop(crossprod(blup_null_V - y2)/length(y2))
}
}
